{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed0910d",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance Rust code from Python code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdf1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    raise RuntimeError(\"OPENROUTER_API_KEY not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c07c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter (ALL cloud models)\n",
    "openrouter = OpenAI(\n",
    "    api_key=openrouter_api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama (local models, optional)\n",
    "ollama = OpenAI(\n",
    "    api_key=\"ollama\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models (OpenRouter names)\n",
    "# ===============================\n",
    "models = [\n",
    "    \"openai/gpt-5\",\n",
    "    \"anthropic/claude-sonnet-4.5\",\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"x-ai/grok-4\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "    \"qwen/qwen3-coder-30b-a3b-instruct\",\n",
    "]\n",
    "\n",
    "# One client for all OpenRouter models\n",
    "clients = {model: openrouter for model in models}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a263056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_info import retrieve_system_info, rust_toolchain_info\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "rust_info = rust_toolchain_info()\n",
    "rust_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a Rust compiler to compile a single rust file called main.rs and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install a Rust toolchain to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile Rust code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "```python\n",
    "compile_command = # something here - to achieve the fastest possible runtime performance\n",
    "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "run_command = # something here\n",
    "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "return run_result.stdout\n",
    "```\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "Have the maximum possible runtime performance in mind; compile time can be slow. Fastest possible runtime performance for this platform is key.\n",
    "Reply with the commands in markdown.\n",
    "\n",
    "System information:\n",
    "{system_info}\n",
    "\n",
    "Rust toolchain information:\n",
    "{rust_info}\n",
    "\"\"\"\n",
    "\n",
    "response = openrouter.chat.completions.create(model=models[0], messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdbf2c",
   "metadata": {},
   "source": [
    "## PLEASE NOTE:\n",
    "\n",
    "We will be writing a solution to convert Python into efficient, optimized Rust code for your machine, which can be compiled to native machine code and executed.\n",
    "\n",
    "It is not necessary for you to execute the code yourself \n",
    "\n",
    "But if you would like to (because it's satisfying!) then I'm including the steps here. Very optional!\n",
    "\n",
    "As an alternative, this is a  website where you can run the Rust code. https://www.programiz.com/rust/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_command = [\n",
    "    \"/Users/ed/.cargo/bin/rustc\",\n",
    "    \"main.rs\",\n",
    "    \"-C\", \"opt-level=3\",\n",
    "    \"-C\", \"target-cpu=native\",\n",
    "    \"-C\", \"codegen-units=1\",\n",
    "    \"-C\", \"lto=fat\",\n",
    "    \"-C\", \"panic=abort\",\n",
    "    \"-C\", \"strip=symbols\",\n",
    "    \"-o\", \"main\",\n",
    "]\n",
    "\n",
    "run_command = [\"./main\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e093cba",
   "metadata": {},
   "source": [
    "## And now, on with the main task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Rust\" \n",
    "extension = \"rs\" if language == \"Rust\" else \"cpp\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "Your task is to convert Python code into high performance {language} code.\n",
    "Respond only with {language} code. Do not provide any explanation other than occasional comments.\n",
    "The {language} response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to {language} with the fastest possible implementation that produces identical output in the least time.\n",
    "The system information is:\n",
    "{system_info}\n",
    "Your response will be written to a file called main.{language} and then compiled and executed; the compilation command is:\n",
    "{compile_command}\n",
    "Respond only with {language} code.\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(code):\n",
    "    with open(f\"main.{extension}\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(model, python):\n",
    "    client = clients[model]\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```rust','').replace('```','')\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer\n",
    "\n",
    "    try:\n",
    "        exec(code, globals_dict)\n",
    "        output = buffer.getvalue()\n",
    "    except Exception as e:\n",
    "        output = f\"Error: {e}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the commands from GPT 5\n",
    "\n",
    "def compile_and_run(code):\n",
    "    write_output(code)\n",
    "    try:\n",
    "        subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "        run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large numbers\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles import CSS\n",
    "\n",
    "with gr.Blocks(css=CSS, theme=gr.themes.Monochrome(), title=f\"Port from Python to {language}\") as ui:\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python = gr.Code(\n",
    "                label=\"Python (original)\",\n",
    "                value=python_hard,\n",
    "                language=\"python\",\n",
    "                lines=26\n",
    "            )\n",
    "        with gr.Column(scale=6):\n",
    "            cpp = gr.Code(\n",
    "                label=f\"{language} (generated)\",\n",
    "                value=\"\",\n",
    "                language=\"cpp\",\n",
    "                lines=26\n",
    "            )\n",
    "\n",
    "    with gr.Row(elem_classes=[\"controls\"]):\n",
    "        python_run = gr.Button(\"Run Python\", elem_classes=[\"run-btn\", \"py\"])\n",
    "        model = gr.Dropdown(models, value=models[0], show_label=False)\n",
    "        convert = gr.Button(f\"Port to {language}\", elem_classes=[\"convert-btn\"])\n",
    "        cpp_run = gr.Button(f\"Run {language}\", elem_classes=[\"run-btn\", \"cpp\"])\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python_out = gr.TextArea(label=\"Python result\", lines=8, elem_classes=[\"py-out\"])\n",
    "        with gr.Column(scale=6):\n",
    "            cpp_out = gr.TextArea(label=f\"{language} result\", lines=8, elem_classes=[\"cpp-out\"])\n",
    "\n",
    "    convert.click(fn=port, inputs=[model, python], outputs=[cpp])\n",
    "    python_run.click(fn=run_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(fn=compile_and_run, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsolution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
